{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作業目的: 熟練自定義collate_fn與sampler進行資料讀取\n",
    "\n",
    "本此作業主要會使用[IMDB](http://ai.stanford.edu/~amaas/data/sentiment/)資料集利用Pytorch的Dataset與DataLoader進行\n",
    "客製化資料讀取。\n",
    "下載後的資料有分成train與test，因為這份作業目的在讀取資料，所以我們取用train部分來進行練習。\n",
    "(請同學先行至IMDB下載資料)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/admin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/admin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import torch and other required modules\n",
    "import glob\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords') #下載stopwords\n",
    "nltk.download('punkt') #下載word_tokenize需要的corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 探索資料與資料前處理\n",
    "這份作業我們使用test資料中的pos與neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length before removing stopwords: 89527\n",
      "vocab length after removing stopwords: 89356\n"
     ]
    }
   ],
   "source": [
    "# 讀取字典，這份字典為review內所有出現的字詞\n",
    "###<your code>###\n",
    "\n",
    "# 以nltk stopwords移除贅字，過多的贅字無法提供有用的訊息，也可能影響模型的訓練\n",
    "print(f\"vocab length before removing stopwords: {len(vocab)}\")\n",
    "### <your code> ###\n",
    "print(f\"vocab length after removing stopwords: {len(vocab)}\")\n",
    "\n",
    "# 將字典轉換成dictionary\n",
    "### <your code> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('./imdb/pos/4715_9.txt', 1), ('./imdb/pos/1930_9.txt', 1)]\n",
      "Total reviews: 25000\n"
     ]
    }
   ],
   "source": [
    "# 將資料打包成(x, y)配對，其中x為review的檔案路徑，y為正評(1)或負評(0)\n",
    "# 這裡將x以檔案路徑代表的原因是讓同學練習不一次將資料全讀取進來，若電腦記憶體夠大(所有資料檔案沒有很大)\n",
    "# 可以將資料全一次讀取，可以減少在訓練時I/O時間，增加訓練速度\n",
    "\n",
    "### <your code> ###\n",
    "print(review_pairs[:2])\n",
    "print(f\"Total reviews: {len(review_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立Dataset, DataLoader, Sampler與Collate_fn讀取資料\n",
    "這裡我們會需要兩個helper functions，其中一個是讀取資料與清洗資料的函式(load_review)，另外一個是生成詞向量函式\n",
    "(generate_vec)，注意這裡我們用來產生詞向量的方法是單純將文字tokenize(為了使產生的文本長度不同，而不使用BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_review(review_path):\n",
    "    \n",
    "    ###<your code>###\n",
    "    \n",
    "\n",
    "def generate_vec(review, vocab_dic):\n",
    "    ### <your code> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立客製化dataset\n",
    "\n",
    "class dataset(Dataset):\n",
    "    '''custom dataset to load reviews and labels\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_pairs: list\n",
    "        directory of all review-label pairs\n",
    "    vocab: list\n",
    "        list of vocabularies\n",
    "    '''\n",
    "    ### <your code> ###\n",
    "    \n",
    "\n",
    "#建立客製化collate_fn，將長度不一的文本pad 0 變成相同長度\n",
    "def collate_fn(batch):\n",
    "\n",
    "    ### <your code> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[80424., 41155., 30845., 85492., 59605., 49711., 29043.,  7568., 13118.,\n",
       "          40831., 12073., 28081., 10508., 22054., 56983., 78328., 56823., 15014.,\n",
       "          67782., 15564., 31903., 63330., 39436., 75912., 68661., 40668., 82191.,\n",
       "           7236., 64709., 19607., 80448., 63831., 11746., 55593., 77823., 34129.,\n",
       "          12797., 66610., 55594., 78528., 19462., 73493.,  8973.,  2663., 18596.,\n",
       "           1824., 22397., 32958., 83571., 50114., 83956., 25894., 36567., 24019.,\n",
       "          14862., 56482.,  3429., 11614., 68173., 74748., 75275., 42096., 15959.,\n",
       "          84522., 23116.,  5133., 32976., 27223., 12826., 29906., 64735., 80321.,\n",
       "          52160., 46420., 51823.,  7429.,  5504., 79965., 23309., 19149., 39495.,\n",
       "          39342., 52321., 85389., 43716., 57914.,  8473., 55641., 85395., 73870.,\n",
       "          35155., 54075., 78047., 75443., 47321., 11454.,  4986., 60386., 45870.,\n",
       "          33681., 77879., 41944.,  9175.,  8302., 17197., 21615., 66824., 37112.,\n",
       "          34375., 18082., 57933., 44989., 88082., 79646., 21966., 33199., 14537.,\n",
       "          34739., 89124., 85064., 74064., 47345., 44471., 27277., 21815., 47349.,\n",
       "          51862., 81423.,  5194., 60590., 63580., 76690., 78621., 35079.,  2944.,\n",
       "          47530., 62363., 36629., 21130., 39031., 50202.,  4160., 30793., 20449.,\n",
       "          34066., 39234., 10814.,  2239., 14756., 33055., 56569., 70408., 70409.,\n",
       "          20249., 43056., 35446., 51735., 54698., 53408., 38208., 56576., 24620.,\n",
       "          63272., 40266., 34780.,  1222.,  6985., 31164., 68999., 40609., 12217.,\n",
       "           4349., 15705., 82304., 56420., 70605., 31352., 75189., 40437., 26172.,\n",
       "          47929., 66873., 27501., 32886., 40443., 57132., 88842., 22347.,  8555.,\n",
       "          49495., 84440., 54878., 67063.,  3861., 59586., 70960., 63643., 39766.,\n",
       "          48607., 11549., 47230., 44356.,  8934., 12573., 88859., 64847.,  1253.],\n",
       "         [88353., 10508., 44908., 40138., 40668., 63831., 72556., 41547., 39635.,\n",
       "           2662., 19465., 22397., 12976., 21739., 75262., 62096., 32958.,  7251.,\n",
       "          25394., 83956., 15948., 85191.,  5124., 56483., 21425., 82383., 68901.,\n",
       "          75111., 40524., 78391., 67319., 29919.,  1677., 24049., 78919.,  3269.,\n",
       "          60386., 72238.,  4459., 56365., 42512., 25499., 65272., 43020., 21966.,\n",
       "          29131., 47345., 39896., 42702., 54834., 42885., 50197., 36302., 26977.,\n",
       "          56060., 54698., 62381., 63272., 18856., 74655., 48934.,  4349.,  8362.,\n",
       "          47929.,  4371., 86355., 34093., 75042., 84453., 67766.,  1253., 88155.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.],\n",
       "         [16765.,  7568.,  3190.,  6853., 57849., 89191., 55416., 22056., 12085.,\n",
       "          25365., 68661., 40668., 74354., 71178., 24849., 63831., 79210.,  2663.,\n",
       "           6875., 34312., 30377., 76801., 18224., 60346., 21739., 67804.,   404.,\n",
       "          83571., 40688., 75091., 76961., 21751., 13325.,  7423., 50722., 33986.,\n",
       "          44057., 85390., 19153., 78919.,  2006., 78047., 89096., 31643.,  2010.,\n",
       "          22298., 58287., 10928., 70361.,  2191., 70368.,  1529.,  4315., 50184.,\n",
       "           2213., 42702., 53379., 40767., 47529., 44314., 82281., 21130., 29963.,\n",
       "          74449., 26479., 41999., 44855.,  2787., 54698., 63272., 18856., 66015.,\n",
       "          31164., 18521., 12217., 63764., 75190., 30648., 77239., 46329., 19741.,\n",
       "          55042.,  4371.,  8555., 41677., 42206., 82851., 54878., 48606., 82158.,\n",
       "          26187., 34805., 24647., 73963., 41682., 71479.,   357.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.],\n",
       "         [30845., 68250., 70634., 34375., 13364., 56569., 57232., 10508.,  2449.,\n",
       "          18856., 28252., 38645., 17358., 14537., 85272., 21432., 81332., 76936.,\n",
       "          14295., 47653., 13948., 45764.,  4349., 36328., 30365., 21040., 40958.,\n",
       "          51823., 37170., 47929., 61954., 23888., 42069., 63831., 76426., 55593.,\n",
       "          66294., 79210.,  8555., 73783., 42885., 14887., 77368., 76232., 63643.,\n",
       "          33296., 53214., 40203., 50209.,  9175., 80644.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.]]),\n",
       " tensor([0, 0, 0, 1]),\n",
       " tensor([207,  72,  97,  51]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用Pytorch的RandomSampler來進行indice讀取並建立dataloader\n",
    "### <your code> ###\n",
    "next(iter(custom_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cupoy_env",
   "language": "python",
   "name": "cupoy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
