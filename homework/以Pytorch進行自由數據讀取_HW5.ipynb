{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "cupoy_env",
      "language": "python",
      "name": "cupoy_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "以Pytorch進行自由數據讀取_HW5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZAxeFF1TToN"
      },
      "source": [
        "### 作業目的: 熟練自定義collate_fn與sampler進行資料讀取\n",
        "\n",
        "本此作業主要會使用[IMDB](http://ai.stanford.edu/~amaas/data/sentiment/)資料集利用Pytorch的Dataset與DataLoader進行\n",
        "客製化資料讀取。\n",
        "下載後的資料有分成train與test，因為這份作業目的在讀取資料，所以我們取用train部分來進行練習。\n",
        "(請同學先行至IMDB下載資料)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdrbsWlfTToS"
      },
      "source": [
        "### 載入套件"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLGtArMdTToT",
        "outputId": "186e6a7e-4e77-480d-f7e9-8af3cad7aa16"
      },
      "source": [
        "# Import torch and other required modules\n",
        "import glob\n",
        "import torch\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.datasets import load_svmlight_file\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords') #下載stopwords\n",
        "nltk.download('punkt') #下載word_tokenize需要的corpus"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /Users/admin/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/admin/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDsy4phCTToU"
      },
      "source": [
        "### 探索資料與資料前處理\n",
        "這份作業我們使用test資料中的pos與neg\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIAAITdWTToU",
        "outputId": "eb2156f7-f7cb-4b56-d216-518bb6faea34"
      },
      "source": [
        "# 讀取字典，這份字典為review內所有出現的字詞\n",
        "with open('./imdb/imdb.vocab', 'r') as f:\n",
        "    vocab = f.read()\n",
        "\n",
        "vocab = vocab.split('\\n')\n",
        "\n",
        "# 以nltk stopwords移除贅字，過多的贅字無法提供有用的訊息，也可能影響模型的訓練\n",
        "print(f\"vocab length before removing stopwords: {len(vocab)}\")\n",
        "vocab = list(set(vocab).difference(set(stopwords.words('english'))))\n",
        "print(f\"vocab length after removing stopwords: {len(vocab)}\")\n",
        "\n",
        "# 將字典轉換成dictionary\n",
        "vocab_dic = dict(zip(vocab, range(len(vocab))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab length before removing stopwords: 89527\n",
            "vocab length after removing stopwords: 89356\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_QT74T2TToU",
        "outputId": "7dc011eb-fadc-4a17-c869-a17ba5beaa37"
      },
      "source": [
        "# 將資料打包成(x, y)配對，其中x為review的檔案路徑，y為正評(1)或負評(0)\n",
        "# 這裡將x以檔案路徑代表的原因是讓同學練習不一次將資料全讀取進來，若電腦記憶體夠大(所有資料檔案沒有很大)\n",
        "# 可以將資料全一次讀取，可以減少在訓練時I/O時間，增加訓練速度\n",
        "\n",
        "review_pos = glob.glob(\"./imdb/pos/*.txt\")\n",
        "review_neg = glob.glob(\"./imdb/neg/*.txt\")\n",
        "review_all = review_pos + review_neg\n",
        "y = [1]*len(review_pos) + [0]*len(review_neg)\n",
        "\n",
        "review_pairs = list(zip(review_all, y))\n",
        "print(review_pairs[:2])\n",
        "print(f\"Total reviews: {len(review_pairs)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('./imdb/pos/4715_9.txt', 1), ('./imdb/pos/1930_9.txt', 1)]\n",
            "Total reviews: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wq7iJHWoTToV"
      },
      "source": [
        "### 建立Dataset, DataLoader, Sampler與Collate_fn讀取資料\n",
        "這裡我們會需要兩個helper functions，其中一個是讀取資料與清洗資料的函式(load_review)，另外一個是生成詞向量函式\n",
        "(generate_vec)，注意這裡我們用來產生詞向量的方法是單純將文字tokenize(為了使產生的文本長度不同，而不使用BoW)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVyiVOQ6TToV"
      },
      "source": [
        "def load_review(review_path):\n",
        "    \n",
        "    with open(review_path, 'r') as f:\n",
        "        review = f.read()\n",
        "        \n",
        "    #移除non-alphabet符號、贅字與tokenize\n",
        "    review = re.sub('[^a-zA-Z]',' ',review)\n",
        "    review = nltk.word_tokenize(review)\n",
        "    review = list(set(review).difference(set(stopwords.words('english'))))\n",
        "    \n",
        "    return review\n",
        "\n",
        "def generate_vec(review, vocab_dic):\n",
        "    doc_vec = []\n",
        "    for word in review:\n",
        "        if vocab_dic.get(word):\n",
        "            doc_vec.append(vocab_dic.get(word))\n",
        "            \n",
        "    return torch.tensor(doc_vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb8AnnTfTToV"
      },
      "source": [
        "#建立客製化dataset\n",
        "\n",
        "class dataset(Dataset):\n",
        "    '''custom dataset to load reviews and labels\n",
        "    Parameters\n",
        "    ----------\n",
        "    data_pairs: list\n",
        "        directory of all review-label pairs\n",
        "    vocab: list\n",
        "        list of vocabularies\n",
        "    '''\n",
        "    def __init__(self, data_dirs, vocab):\n",
        "        self.data_dirs = data_dirs\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_dirs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pair = self.data_dirs[idx]\n",
        "        review = pair[0]\n",
        "        review = load_review(review)\n",
        "        review = generate_vec(review, self.vocab)\n",
        "        \n",
        "        return review, pair[1]\n",
        "    \n",
        "\n",
        "#建立客製化collate_fn，將長度不一的文本pad 0 變成相同長度\n",
        "def collate_fn(batch):\n",
        "\n",
        "    corpus, labels = zip(*batch) \n",
        "    \n",
        "    ### create pads for corpus ###\n",
        "    lengths = [len(x) for x in corpus]\n",
        "    max_length = max(lengths)\n",
        "    \n",
        "    batch_corpus = []\n",
        "    \n",
        "    for i in range(len(corpus)):\n",
        "        # pad corpus\n",
        "        tmp_pads = torch.zeros(max_length)\n",
        "        tmp_pads[:lengths[i]] = corpus[i]\n",
        "        tmp_pads.view(-1, 1)\n",
        "        batch_corpus.append(tmp_pads.view(1,-1))\n",
        "\n",
        "    return torch.cat(batch_corpus,dim=0), torch.tensor(labels) , torch.tensor(lengths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "_4LZiN1_TToV",
        "outputId": "ae3204a8-acac-46e8-957e-01602316116a"
      },
      "source": [
        "# 使用Pytorch的RandomSampler來進行indice讀取並建立dataloader\n",
        "custom_dst = dataset(review_pairs, vocab_dic)\n",
        "custom_dataloader = DataLoader(dataset=custom_dst, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "next(iter(custom_dataloader))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[80424., 41155., 30845., 85492., 59605., 49711., 29043.,  7568., 13118.,\n",
              "          40831., 12073., 28081., 10508., 22054., 56983., 78328., 56823., 15014.,\n",
              "          67782., 15564., 31903., 63330., 39436., 75912., 68661., 40668., 82191.,\n",
              "           7236., 64709., 19607., 80448., 63831., 11746., 55593., 77823., 34129.,\n",
              "          12797., 66610., 55594., 78528., 19462., 73493.,  8973.,  2663., 18596.,\n",
              "           1824., 22397., 32958., 83571., 50114., 83956., 25894., 36567., 24019.,\n",
              "          14862., 56482.,  3429., 11614., 68173., 74748., 75275., 42096., 15959.,\n",
              "          84522., 23116.,  5133., 32976., 27223., 12826., 29906., 64735., 80321.,\n",
              "          52160., 46420., 51823.,  7429.,  5504., 79965., 23309., 19149., 39495.,\n",
              "          39342., 52321., 85389., 43716., 57914.,  8473., 55641., 85395., 73870.,\n",
              "          35155., 54075., 78047., 75443., 47321., 11454.,  4986., 60386., 45870.,\n",
              "          33681., 77879., 41944.,  9175.,  8302., 17197., 21615., 66824., 37112.,\n",
              "          34375., 18082., 57933., 44989., 88082., 79646., 21966., 33199., 14537.,\n",
              "          34739., 89124., 85064., 74064., 47345., 44471., 27277., 21815., 47349.,\n",
              "          51862., 81423.,  5194., 60590., 63580., 76690., 78621., 35079.,  2944.,\n",
              "          47530., 62363., 36629., 21130., 39031., 50202.,  4160., 30793., 20449.,\n",
              "          34066., 39234., 10814.,  2239., 14756., 33055., 56569., 70408., 70409.,\n",
              "          20249., 43056., 35446., 51735., 54698., 53408., 38208., 56576., 24620.,\n",
              "          63272., 40266., 34780.,  1222.,  6985., 31164., 68999., 40609., 12217.,\n",
              "           4349., 15705., 82304., 56420., 70605., 31352., 75189., 40437., 26172.,\n",
              "          47929., 66873., 27501., 32886., 40443., 57132., 88842., 22347.,  8555.,\n",
              "          49495., 84440., 54878., 67063.,  3861., 59586., 70960., 63643., 39766.,\n",
              "          48607., 11549., 47230., 44356.,  8934., 12573., 88859., 64847.,  1253.],\n",
              "         [88353., 10508., 44908., 40138., 40668., 63831., 72556., 41547., 39635.,\n",
              "           2662., 19465., 22397., 12976., 21739., 75262., 62096., 32958.,  7251.,\n",
              "          25394., 83956., 15948., 85191.,  5124., 56483., 21425., 82383., 68901.,\n",
              "          75111., 40524., 78391., 67319., 29919.,  1677., 24049., 78919.,  3269.,\n",
              "          60386., 72238.,  4459., 56365., 42512., 25499., 65272., 43020., 21966.,\n",
              "          29131., 47345., 39896., 42702., 54834., 42885., 50197., 36302., 26977.,\n",
              "          56060., 54698., 62381., 63272., 18856., 74655., 48934.,  4349.,  8362.,\n",
              "          47929.,  4371., 86355., 34093., 75042., 84453., 67766.,  1253., 88155.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.],\n",
              "         [16765.,  7568.,  3190.,  6853., 57849., 89191., 55416., 22056., 12085.,\n",
              "          25365., 68661., 40668., 74354., 71178., 24849., 63831., 79210.,  2663.,\n",
              "           6875., 34312., 30377., 76801., 18224., 60346., 21739., 67804.,   404.,\n",
              "          83571., 40688., 75091., 76961., 21751., 13325.,  7423., 50722., 33986.,\n",
              "          44057., 85390., 19153., 78919.,  2006., 78047., 89096., 31643.,  2010.,\n",
              "          22298., 58287., 10928., 70361.,  2191., 70368.,  1529.,  4315., 50184.,\n",
              "           2213., 42702., 53379., 40767., 47529., 44314., 82281., 21130., 29963.,\n",
              "          74449., 26479., 41999., 44855.,  2787., 54698., 63272., 18856., 66015.,\n",
              "          31164., 18521., 12217., 63764., 75190., 30648., 77239., 46329., 19741.,\n",
              "          55042.,  4371.,  8555., 41677., 42206., 82851., 54878., 48606., 82158.,\n",
              "          26187., 34805., 24647., 73963., 41682., 71479.,   357.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.],\n",
              "         [30845., 68250., 70634., 34375., 13364., 56569., 57232., 10508.,  2449.,\n",
              "          18856., 28252., 38645., 17358., 14537., 85272., 21432., 81332., 76936.,\n",
              "          14295., 47653., 13948., 45764.,  4349., 36328., 30365., 21040., 40958.,\n",
              "          51823., 37170., 47929., 61954., 23888., 42069., 63831., 76426., 55593.,\n",
              "          66294., 79210.,  8555., 73783., 42885., 14887., 77368., 76232., 63643.,\n",
              "          33296., 53214., 40203., 50209.,  9175., 80644.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.]]),\n",
              " tensor([0, 0, 0, 1]),\n",
              " tensor([207,  72,  97,  51]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_RfqwgKTToW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}