{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert_news_classifier_1_hw.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"OhoVqtGd_rkc"},"source":["# 專題（二）：建置Bert新聞分類器之資料集\n","\n","## 專案目標\n","- 目標：請試著建製 BertForSequenceClassification 看得懂的資料集 NewsDataset\n","- news_clustering_train.tsv 中有 1800 篇新聞，六種類別的新聞各 300 篇\n","- news_clustering_test.tsv 中有 600 篇新聞，六種類別的新聞各 100 篇\n","- 六種類別：體育、財經、科技、旅遊、農業、遊戲\n","\n","## 實作提示\n","- STEP1：從 news_clustering_train.tsv 和 news_clustering_test.tsv 中取出標題和類別\n","- STEP2：繼承 torch.utils.data.Dataset 並實作 NewsDataset，其中需要用到 bert tokenizer (請參考官方對BertForSequenceClassification的說明)\n","- STEP3：因為每一個從 NewsDataset 來的樣本長度都不一樣，所以需要實作 collate_fn，來zero padding 到同一序列長度\n","- STEP4：使用 torch.utils.data.DataLoader 來創造 train_loader和valid_loader\n","\n","## 重要知識點：專題結束後你可以學會\n","- 如何讀取並處理 NLP 資料，產生可以適用 BertForSequenceClassification 的資料集\n","- 了解 BERT 的 Sequence Classification 任務如何進行"]},{"cell_type":"code","metadata":{"id":"2GDmjOyYMklS","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"d5e54e35-effb-4ba7-b0c0-b3ea0c9c328a"},"source":["!python --version"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Python 3.6.9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1JkVfTP5Qn-1"},"source":["!pip install -q transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WKW9s5YtMvop"},"source":["import pandas as pd\n","import numpy as np\n","\n","import torch\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","\n","from transformers import BertTokenizer, BertForSequenceClassification"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i8OlmfVQM7UW"},"source":["df_train = pd.read_csv('news_clustering_train.tsv', sep='\\t')\n","df_test = pd.read_csv('news_clustering_test.tsv', sep='\\t')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ivoJVDvnM_Ok"},"source":["train_titles = {row['index']: row['title'] for _, row in df_train.iterrows()}\n","train_classes = {row['index']: row['class'] for _, row in df_train.iterrows()}\n","\n","valid_titles = {row['index']: row['title'] for _, row in df_test.iterrows()}\n","valid_classes = {row['index']: row['class'] for _, row in df_test.iterrows()}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QM8vITkwNMJP"},"source":["ALL_NEWS_CLASSES = ['體育', '財經', '科技', '旅遊', '農業', '遊戲']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ACnCRl2xNV3r"},"source":["MODEL_NAME = 'bert-base-chinese'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rK1yhgITcL3Q"},"source":["# 建立數據集\n","class NewsDataset(Dataset):\n","    def __init__(self, tokenizer, titles, classes):\n","        self.tokenizer = tokenizer\n","        self.indexes = []\n","        self.texts = []\n","        self.labels = []\n","        for index in titles:\n","            self.indexes.append(index)\n","            self.texts.append(titles[index])\n","            self.labels.append(classes[index])\n","\n","    def __getitem__(self, idx):\n","        text = self.texts[idx]\n","\n","        input = self.tokenizer(text, return_tensors='pt')\n","        label = torch.tensor(ALL_NEWS_CLASSES.index(self.labels[idx]))\n","\n","        return input, label\n","\n","    def __len__(self):\n","        return len(self.indexes)\n","\n","\n","def create_mini_batch(samples):\n","    input_ids = []\n","    token_type_ids = []\n","    attention_mask = []\n","    labels = []\n","    for s in samples:\n","        input_ids.append(s[0]['input_ids'].squeeze(0))\n","        token_type_ids.append(s[0]['token_type_ids'].squeeze(0))\n","        attention_mask.append(s[0]['attention_mask'].squeeze(0))\n","        labels.append(s[1])\n","\n","    # zero pad 到同一序列長度\n","    # Code Here\n","\n","    # End\n"," \n","    labels = torch.stack(labels)\n","\n","    return input_ids, token_type_ids, attention_mask, labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4cmG8VcfeiNN"},"source":["batch_size = 32\n","\n","tokenizer = # Code Here\n","\n","train_dataset = NewsDataset(tokenizer, train_titles, train_classes)\n","valid_dataset = NewsDataset(tokenizer, valid_titles, valid_classes)\n","\n","train_loader = DataLoader(\n","    dataset=train_dataset,\n","    batch_size=batch_size,\n","    collate_fn=create_mini_batch,\n","    shuffle=True)\n","valid_loader = DataLoader(\n","    dataset=valid_dataset,\n","    batch_size=batch_size,\n","    collate_fn=create_mini_batch)"],"execution_count":null,"outputs":[]}]}