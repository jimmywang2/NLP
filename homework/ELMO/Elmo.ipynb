{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作業 : 調整 ELMo 模型的不同訓練參數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [作業目標]\n",
    "- 調整 ELMo 模型的不同參數, 分別觀察 loss 數據並比較"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [參數說明]\n",
    "- UNITS : LSTM 特徵維度，即每一筆訓練輸入單字的個數\n",
    "- N_LAYERS : LSTM 堆疊的層數\n",
    "- BATCH_SIZE : 訓練批次大小，即幾筆資料合併做一次倒傳遞\n",
    "- LEARNING_RATE : 學習速率，影響收斂的快慢，須配合 BATCH_SIZE 調整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [參考解答]\n",
    "- UNITS : 數值越大loss越小，但是會與計算時間成正比，且改善降幅會越來越小\n",
    "- N_LAYERS : 超過兩層以上時，影響不大\n",
    "- BATCH_SIZE : 影響收斂所需的迭代次數，也會影響最終能收斂到的 loss 下限\n",
    "- LEARNING_RATE : 配合BATCH_SIZE調整，不適當的學習速率也會影響最終收斂的下限"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Deep contextualized word representations](https://arxiv.org/pdf/1802.05365.pdf)\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import utils    # this refers to utils.py in my [repo](https://github.com/MorvanZhou/NLP-Tutorials/)\n",
    "import time\n",
    "import os\n",
    "\n",
    "class ELMo(keras.Model):\n",
    "    def __init__(self, v_dim, emb_dim, units, n_layers, lr):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.units = units\n",
    "\n",
    "        # encoder\n",
    "        self.word_embed = keras.layers.Embedding(\n",
    "            input_dim=v_dim, output_dim=emb_dim,  # [n_vocab, emb_dim]\n",
    "            embeddings_initializer=keras.initializers.RandomNormal(0., 0.001),\n",
    "            mask_zero=True,\n",
    "        )\n",
    "        # forward lstm\n",
    "        self.fs = [keras.layers.LSTM(units, return_sequences=True) for _ in range(n_layers)]\n",
    "        self.f_logits = keras.layers.Dense(v_dim)\n",
    "        # backward lstm\n",
    "        self.bs = [keras.layers.LSTM(units, return_sequences=True, go_backwards=True) for _ in range(n_layers)]\n",
    "        self.b_logits = keras.layers.Dense(v_dim)\n",
    "\n",
    "        self.cross_entropy1 = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        self.cross_entropy2 = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        self.opt = keras.optimizers.Adam(lr)\n",
    "\n",
    "    def call(self, seqs):\n",
    "        embedded = self.word_embed(seqs)        # [n, step, dim]\n",
    "        \"\"\"\n",
    "        0123    forward\n",
    "        1234    forward predict\n",
    "         1234   backward \n",
    "         0123   backward predict\n",
    "        \"\"\"\n",
    "        mask = self.word_embed.compute_mask(seqs)\n",
    "        fxs, bxs = [embedded[:, :-1]], [embedded[:, 1:]]\n",
    "        for fl, bl in zip(self.fs, self.bs):\n",
    "            fx = fl(\n",
    "                fxs[-1], mask=mask[:, :-1], initial_state=fl.get_initial_state(fxs[-1])\n",
    "            )           # [n, step-1, dim]\n",
    "            bx = bl(\n",
    "                bxs[-1], mask=mask[:, 1:], initial_state=bl.get_initial_state(bxs[-1])\n",
    "            )  # [n, step-1, dim]\n",
    "            fxs.append(fx)      # predict 1234\n",
    "            bxs.append(tf.reverse(bx, axis=[1]))    # predict 0123\n",
    "        return fxs, bxs\n",
    "\n",
    "    def step(self, seqs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            fxs, bxs = self.call(seqs)\n",
    "            fo, bo = self.f_logits(fxs[-1]), self.b_logits(bxs[-1])\n",
    "            loss = (self.cross_entropy1(seqs[:, 1:], fo) + self.cross_entropy2(seqs[:, :-1], bo))/2\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return loss, (fo, bo)\n",
    "\n",
    "    def get_emb(self, seqs):\n",
    "        fxs, bxs = self.call(seqs)\n",
    "        xs = [tf.concat((f[:, :-1, :], b[:, 1:, :]), axis=2).numpy() for f, b in zip(fxs, bxs)]\n",
    "        for x in xs:\n",
    "            print(\"layers shape=\", x.shape)\n",
    "        return xs\n",
    "\n",
    "\n",
    "def train(model, data, step):\n",
    "    t0 = time.time()\n",
    "    for t in range(step):\n",
    "        seqs = data.sample(BATCH_SIZE)\n",
    "        loss, (fo, bo) = model.step(seqs)\n",
    "        if t % 80 == 0:\n",
    "            fp = fo[0].numpy().argmax(axis=1)\n",
    "            bp = bo[0].numpy().argmax(axis=1)\n",
    "            t1 = time.time()\n",
    "            print(\n",
    "                \"\\n\\nstep: \", t,\n",
    "                \"| time: %.2f\" % (t1 - t0),\n",
    "                \"| loss: %.3f\" % loss.numpy(),\n",
    "                \"\\n| tgt: \", \" \".join([data.i2v[i] for i in seqs[0] if i != data.pad_id]),\n",
    "                \"\\n| f_prd: \", \" \".join([data.i2v[i] for i in fp if i != data.pad_id]),\n",
    "                \"\\n| b_prd: \", \" \".join([data.i2v[i] for i in bp if i != data.pad_id]),\n",
    "                )\n",
    "            t0 = t1\n",
    "    os.makedirs(\"./visual/models/elmo\", exist_ok=True)\n",
    "    model.save_weights(\"./visual/models/elmo/model.ckpt\")\n",
    "\n",
    "\n",
    "def export_w2v(model, data):\n",
    "    model.load_weights(\"./visual/models/elmo/model.ckpt\")\n",
    "    emb = model.get_emb(data.sample(4))\n",
    "    print(emb)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    utils.set_soft_gpu(True)\n",
    "    UNITS = 256\n",
    "    N_LAYERS = 2\n",
    "    BATCH_SIZE = 16\n",
    "    LEARNING_RATE = 2e-3\n",
    "    d = utils.MRPCSingle(\"./MRPC\", rows=2000)\n",
    "    print(\"num word: \", d.num_word)\n",
    "    m = ELMo(d.num_word, emb_dim=UNITS, units=UNITS, n_layers=N_LAYERS, lr=LEARNING_RATE)\n",
    "    train(m, d, 10000)\n",
    "    export_w2v(m, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
